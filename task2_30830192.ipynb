{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 2 in Assessment 1\n",
    "#### Student Name: Vyom Chauhan\n",
    "#### Student ID: 30830192\n",
    "\n",
    "Date: 13/09/2020\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 2.7.11 and Jupyter notebook\n",
    "\n",
    " </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is to extract unigrams, bigrams and make a vocab and count vector for the given twitter data present in the excel files. Firstly, we combine the data from different sheets into a single dataframe and then extract only the english tweets from the dataframe. Simultaneously we create a dictionary with date as key and the english tweets as value. Next we import the stopwords file and make a set of stopwords. Next we tokenize the data and store it into a dictionary. For extracting unigrams we firstly remove the stopwords and then stem the remaining tokens using porter stemmer. Next we extract the top 100 unigrams using the FreqDist function and create a string to store the unigrams in a file according to the given format. Next to extract bigrams we use the ngrams function setting n=2 on the tokenized text and extract the top 100 bigrams using the FreqDist function. Next we write it to the bigram file using a string according to the format given. To make a vocabulary of the tweets, firstly we remove the stopwords and then append the tokens into a list to remove the context dependent and rare words. Next, we stem the remaining tokens and remove tokens with length less than 3. Netx to create top 200 bigrams using the pmi measure we use the stopwords removed list and flatten it. We add the bigrams list to the vocab and sort it. Next we write it to the vocab file along with its index. To make the count vectors we use the mwetokenizer instead of the regex tokenizer given so that we can keep the bigrams in the text. Next we perform the operations of removing stopwords, rare words etc. and pass the dictionary as a list to the count vectorizer. Next we create a dictionary of the token index and its count and write it to the countvec file in the format specified. We notice that the vocab has around 11000 words which also consist of the top 200 bigrams. The rare, context dependent and independent words have been removed from the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "from langid import classify\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.util import ngrams\n",
    "from itertools import chain\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# variable to store data in the form of a dictionary\n",
    "twitter_dict = {}\n",
    "# reading the twitter data\n",
    "twitter_data = pd.ExcelFile(\"30830192.xlsx\")\n",
    "# creating the column names\n",
    "new_header = ['text', 'id', 'created_at']\n",
    "# for storing data stored in different sheets\n",
    "for sheet in twitter_data.sheet_names:\n",
    "    # parsing the sheets\n",
    "    twitter_df = twitter_data.parse(sheet)\n",
    "    # dropping columns having all null values\n",
    "    twitter_df = twitter_df.dropna(how='all', axis = 1)\n",
    "    # dropping rows with all null values\n",
    "    twitter_df = twitter_df.dropna(how='all', axis = 0)\n",
    "    # renaming columns\n",
    "    twitter_df.columns = new_header\n",
    "    # ignoring first row if it has column names\n",
    "    if (twitter_df.columns == twitter_df.iloc[0])[1]:\n",
    "        twitter_df = twitter_df[1:]\n",
    "    # filtering english tweets\n",
    "    twitter_df = twitter_df[list(map(lambda x : classify(str(x))[0]=='en', twitter_df['text']))]\n",
    "    # storing a sheet with date as key and text as value\n",
    "    twitter_dict[sheet] = ' '.join(str(x) for x in twitter_df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading stopwords file\n",
    "stop = open(\"stopwords_en.txt\", \"r\")\n",
    "# storing stopwords as a list\n",
    "stopwords_list = stop.read().split()\n",
    "# creating a set of stopwords\n",
    "stopwords_set = set(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing tokenizer\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "# tokenization\n",
    "for key in twitter_dict.keys():\n",
    "    # converting all text to lower case\n",
    "    twitter_dict[key] = twitter_dict[key].lower()\n",
    "    # converting text string to list by using tokenizer\n",
    "    twitter_dict[key] = tokenizer.tokenize(twitter_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying contents of twitter_dict o uni_dict\n",
    "uni_dict = twitter_dict.copy()\n",
    "# porter stemmer object\n",
    "stem = PorterStemmer()\n",
    "# string to be written in file\n",
    "uni_text = ''\n",
    "# making the unigrams\n",
    "for key in uni_dict.keys():\n",
    "    # removing stopwords\n",
    "    uni_dict[key] = [w for w in uni_dict[key] if w not in stopwords_set]\n",
    "    # stemming the tokens\n",
    "    uni_dict[key] = [stem.stem(w) for w in uni_dict[key]]\n",
    "    # storing only the top 100 frequent unigrams\n",
    "    uni_dict[key] = FreqDist(w for w in uni_dict[key]).most_common(100)\n",
    "    # appending items to string\n",
    "    uni_text = uni_text + str(key) + ':' + str(uni_dict[key]) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating text file for writing\n",
    "file = open(\"30830192_100uni.txt\", \"w\")\n",
    "# writing to text file\n",
    "file.write(uni_text) \n",
    "# closing file object\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying contents of twitter_dict to bi_dict\n",
    "bi_dict = twitter_dict.copy()\n",
    "# string to be written in file\n",
    "bi_text = ''\n",
    "# making the bigrams\n",
    "for key in bi_dict.keys():\n",
    "    # extracting n-grams where n=2\n",
    "    bigrams = ngrams(bi_dict[key], n=2)\n",
    "    # storing only the top 100 frequent bigrams\n",
    "    bi_dict[key] = FreqDist(bigrams).most_common(100)\n",
    "    # appending items to string\n",
    "    bi_text = bi_text + str(key) + ':' + str(bi_dict[key]) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating text file for writing\n",
    "file = open(\"30830192_100bi.txt\", \"w\") \n",
    "# writing to text file\n",
    "file.write(bi_text) \n",
    "# closing file object\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# copying contents of twitter_dict to another dictionary\n",
    "words_dict = twitter_dict.copy()\n",
    "# stores stopwords removed unigrams\n",
    "uni_list = []\n",
    "# removing stpwords\n",
    "for key in words_dict.keys():\n",
    "    words_dict[key] = [w for w in words_dict[key] if w not in stopwords_set]\n",
    "#     words_dict[key] = [stem.stem(w) for w in words_dict[key]]\n",
    "#     words_dict[key] = [w for w in words_dict[key] if len(w) > 2]\n",
    "    uni_list.append(words_dict[key])\n",
    "\n",
    "# stores frequency distribution for words\n",
    "# did word appear on a particular date or not\n",
    "# even if it appears multiple times on a date, freq is still 1\n",
    "# freq increases only if it appears on another date\n",
    "freq_dict = FreqDist(list(chain.from_iterable(set(x) for x in uni_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for context dependent words i.e words with freq > 60\n",
    "freq_context = {k:v for (k,v) in freq_dict.items() if v > 60}\n",
    "# dictionary for rare words i.e words with freq < 5\n",
    "freq_rare = {k:v for (k,v) in freq_dict.items() if v < 5}\n",
    "\n",
    "# cleaning the dictionary further\n",
    "for key in words_dict.keys():\n",
    "    # removing the context dependent words\n",
    "    words_dict[key] = [w for w in words_dict[key] if w not in freq_context.keys()]\n",
    "    # removing rare words\n",
    "    words_dict[key] = [w for w in words_dict[key] if w not in freq_rare.keys()]\n",
    "    # stemming words \n",
    "    words_dict[key] = [stem.stem(w) for w in words_dict[key]]\n",
    "    # removing words with length < 3\n",
    "    words_dict[key] = [w for w in words_dict[key] if len(w)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store vocab\n",
    "vocab = []\n",
    "# adding words in dictionary to vocab \n",
    "for (key, value) in (words_dict.items()):\n",
    "         for item in value:\n",
    "            # removing duplicates\n",
    "            if item not in vocab:\n",
    "                vocab.append(item) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the stopwords removed dictionary in a list and flattening the list\n",
    "flat_list = [item for sublist in uni_list for item in sublist]\n",
    "# generating top 200 bigrams using pmi measure\n",
    "b_m = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(flat_list)\n",
    "bigrams = finder.nbest(b_m.pmi, 200)\n",
    "# storing the bigrams into a list and joining them using _\n",
    "bigram_list = []\n",
    "for item in bigrams:\n",
    "    # joining the bigrams with _\n",
    "    bigram_list.append(item[0] + \"_\" + item[1])\n",
    "# appending bigrams to the vocab and sorting list\n",
    "vocab += bigram_list\n",
    "vocab = sorted(vocab)\n",
    "# string for writing to file\n",
    "vocab_txt = ''\n",
    "# final vocab with index of words\n",
    "for item in vocab:\n",
    "    vocab_txt += item + ':' + str(vocab.index(item)) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating text file for writing\n",
    "file = open(\"30830192_vocab.txt\", \"w\")\n",
    "# writing to text file\n",
    "file.write(vocab_txt) \n",
    "# closing file object\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the mwetokenizer\n",
    "tokenizer = MWETokenizer()\n",
    "# passing the top 200 bigrams to the tokenizer\n",
    "mwetokenizer = MWETokenizer(bigrams)\n",
    "# makes sure that the top 200 bigrams passed remain in the dictionary\n",
    "mwe_dict = dict((key, mwetokenizer.tokenize(text)) for key, text in twitter_dict.items())\n",
    "# processing the mwetokenized dictionary\n",
    "for key in mwe_dict.keys():\n",
    "    # removing stopwords\n",
    "    mwe_dict[key] = [w for w in mwe_dict[key] if w not in stopwords_set]\n",
    "    # removing context dependent words\n",
    "    mwe_dict[key] = [w for w in mwe_dict[key] if w not in freq_context.keys()]\n",
    "    # removing rare words\n",
    "    mwe_dict[key] = [w for w in mwe_dict[key] if w not in freq_rare.keys()]\n",
    "    # stemming the words\n",
    "    mwe_dict[key] = [stem.stem(w) for w in mwe_dict[key]]\n",
    "    # removing words with length < 3\n",
    "    mwe_dict[key] = [w for w in mwe_dict[key] if len(w)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list to pass to transform function\n",
    "text_list = []\n",
    "# appeding all tokens of a date to one item of a list\n",
    "for (key, tokens) in mwe_dict.items():\n",
    "    a = ' '.join(tokens)\n",
    "    text_list.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the count vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# passing the list to the count vector function\n",
    "count_vector = vectorizer.fit_transform(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a nested dictionary having word and frequency for a date\n",
    "count_dict = {}\n",
    "# to traverse the count_vector array\n",
    "c = 0\n",
    "# extracting words from the vectorizer\n",
    "vocab2 = vectorizer.get_feature_names()\n",
    "\n",
    "# making a nested dictionary\n",
    "for key in mwe_dict.keys(): \n",
    "    count_d[key] = {'':''}\n",
    "    # showing the frequency of a word or bigram only if it is in the vocab defined earlier\n",
    "    for (word, count) in zip(vocab2, count_vector.toarray()[c]):\n",
    "        # if count = 0 don't add the word to dictionary\n",
    "        if word in vocab and count > 0:\n",
    "            count_d[key].update({vocab.index(word):count})\n",
    "    count_d[key].pop('', None)\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the count_vector to file in the format specified\n",
    "with open('30830192_countVec.txt','w') as file:\n",
    "    for key, value in count_dict.items():\n",
    "        f.write(key + ',')\n",
    "        f.write(\",\".join([\"{}:{}\".format(word, count) for word, count in value.items()]))\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
