{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 1 in Assessment 1\n",
    "#### Student Name: Vyom Chauhan\n",
    "#### Student ID: 30830192\n",
    "\n",
    "Date: 13/09/2020\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 2.7.11 and Jupyter notebook\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is to extract the english tweets from the given twitter data and output them as a file in xml format. Firstly we read all the files from the directory 30830192 and append it to a string. Next we use the findall function from the re library to extract text, date and id of the tweets. Regular expression r'\"text\":\".*?\"' - extracts all strings beginning with \"text\":\" and having whatever characters and ending with \". Regular expression r'\"id\":\"\\d{19}\"' - extracts all strings beginning with \"id\":\" and having 19 digits and ending with \". Regular expression r'\"created_at\":\"\\d{4}-\\d{2}-\\d{2}' - extracts all strings beginning with \"created_at\":\" and having date in the format xxxx-xx-xx. Next we clean the text, date and id and mofify it to the desired xml format and classify them extracting only the english texts. Next we create a dictionary by zipping the three lists and then update the dictionary using a for loop. We store the final output as a string  modified into the xml format and then write it to the output file. On parsing the output file it matches the desired xml format as required for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import os,re\n",
    "from langid import classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable to store the text from files\n",
    "text = \"\"\n",
    "# accessing the directory '30830192' which has tweet data\n",
    "for root, dirs, files in os.walk(\"30830192\"):\n",
    "    # accessing the files from directory part1\n",
    "    for file in files:\n",
    "        with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "            # reading the file\n",
    "            data = f.read()\n",
    "            # appending file data to a string\n",
    "            text += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts all strings beginning with \"text\":\" and having whatever characters and ending with \"\n",
    "tweet = re.findall(r'\"text\":\".*?\"', text)\n",
    "# extracts all strings beginning with \"id\":\" and having 19 digits and ending with \"\n",
    "data_id = re.findall(r'\"id\":\"\\d{19}\"', text)\n",
    "# extracts all strings beginning with \"created_at\":\" and having date in the format xxxx-xx-xx\n",
    "date = re.findall(r'\"created_at\":\"\\d{4}-\\d{2}-\\d{2}', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists to store english tweets and their corresponding dates and ids\n",
    "tweet_en = []\n",
    "date_en = []\n",
    "id_en = []\n",
    "# cleaning the data and extracting english tweets\n",
    "for i in range(len(tweet)):\n",
    "    # removing \"text\":\" and extra \"\n",
    "    tweet[i] = tweet[i][8:-1]\n",
    "    # removing \"created_at\":\" \n",
    "    date[i] = date[i][14:]\n",
    "    # removing \"id\" and extra \"\n",
    "    data_id[i] = data_id[i][5:]\n",
    "    # adding special characters for xml format\n",
    "    mod = \"\\\"\" + date[i] + \"\\\"\"\n",
    "    date[i] = \"tweets date=\" + mod\n",
    "    data_id[i] = \"tweet id=\" + data_id[i]\n",
    "    # extracting english text and their corresponding date and id while removing duplicate id's\n",
    "    if classify(tweet[i])[0] == 'en' and data_id[i] not in id_en:\n",
    "        tweet_en.append(tweet[i])\n",
    "        date_en.append(date[i])\n",
    "        id_en.append(data_id[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a nested dictionary with date as the parent key and id as the child key with the tweet as value\n",
    "data_dic = {str(a): {str(b): str(c)} for a, b, c in zip(date_en, id_en, tweet_en)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editing data_dic since it only contains one tweet for a date  \n",
    "for key, nested_dict in data_dic.items():\n",
    "    for (d, i, t) in zip(date_en, id_en, tweet_en):\n",
    "        if key == d:\n",
    "            # updating data_dic if the tweet id doesn't exist for that date\n",
    "            if not(i in nested_dict):\n",
    "                data_dic[key].update({i:t})         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the dictionary into the xml format\n",
    "final_xml = \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\" + \"\\n\" + \"<data>\" + \"\\n\"\n",
    "for date, nested_dict in data_dic.items():\n",
    "    final_xml += \"<\" + date + \">\\n\"\n",
    "    for tweet_id in sorted(nested_dict):\n",
    "        final_xml += \"<\" + tweet_id + \">\" + nested_dict[tweet_id] + \"</tweet>\\n\"\n",
    "    final_xml += \"</tweets>\\n\"\n",
    "final_xml += \"</data>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating xml file for writing\n",
    "file = open(\"30830192.xml\", \"w\", encoding='utf-8') \n",
    "# writing to xml file\n",
    "file.write(final_xml) \n",
    "# closing file object\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with open('30830192.xml', encoding='utf-8') as fd:\n",
    "#     doc = xmltodict.parse(fd.read())\n",
    "# doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
